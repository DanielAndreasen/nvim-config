return {
--     "yetone/avante.nvim",
--     -- event = "VeryLazy",
--     version = false, -- Set this to "*" to always pull the latest release version, or set it to false to update to the latest code changes.
--     opts = {
--         provider = "mistral", -- "claude" or "openai" or "azure"
--         vendors = {
--             mistral = {
--                 api_key_name = "MISTRAL_API_KEY",
--                 endpoint = "https://api.mistral.ai/v1/chat/completions",
--                 model = "mistral-large-latest",
--                 parse_curl_args = function(opts, code_opts)
--                     local headers = {
--                         ["Content-Type"] = "application/json",
--                         ["Accept"] = "application/json",
--                         ["Authorization"] = "Bearer " .. os.getenv(opts.api_key_name)
--                     }
--
--                     local Llm = require("avante.providers")
--
--                     return {
--                         url = opts.endpoint,
--                         -- timeout = base.timeout,
--                         insecure = false,
--                         headers = headers,
--                         body = vim.tbl_deep_extend(
--                             "force",
--                             {
--                                 model = opts.model,
--                                 temperature = 0,
--                                 topK = -1,
--                                 topP = -1,
--                                 maxTokensToSample = 4000,
--                                 stream = true,
--                                 messages = Llm.openai.parse_messages(code_opts)
--                             },
--                             {}
--                         )
--                     }
--                 end,
--                 is_o_series_model = function() return true end,
--                 parse_response = function(data_stream, event_state, opts)
--                     local Llm = require("avante.providers")
--                     Llm.openai.parse_response(data_stream, event_state, opts)
--                 end
--             }
--         }
--     },
--     build = "make",
--     dependencies = {
--         "nvim-treesitter/nvim-treesitter",
--         "stevearc/dressing.nvim",
--         "nvim-lua/plenary.nvim",
--         "MunifTanjim/nui.nvim",
--         --- The below dependencies are optional,
--         "echasnovski/mini.pick", -- for file_selector provider mini.pick
--         "nvim-telescope/telescope.nvim", -- for file_selector provider telescope
--         "hrsh7th/nvim-cmp", -- autocompletion for avante commands and mentions
--         "ibhagwan/fzf-lua", -- for file_selector provider fzf
--         "nvim-tree/nvim-web-devicons", -- or echasnovski/mini.icons
--         "zbirenbaum/copilot.lua", -- for providers='copilot'
--         {
--             -- support for image pasting
--             "HakonHarnes/img-clip.nvim",
--             event = "VeryLazy",
--             opts = {
--                 -- recommended settings
--                 default = {
--                     embed_image_as_base64 = false,
--                     prompt_for_file_name = false,
--                     drag_and_drop = {
--                         insert_mode = true
--                     },
--                     -- required for Windows users
--                     use_absolute_path = true
--                 }
--             }
--         },
--         {
--             -- Make sure to set this up properly if you have lazy=true
--             "MeanderingProgrammer/render-markdown.nvim",
--             opts = {
--                 file_types = {"markdown", "Avante"}
--             },
--             ft = {"markdown", "Avante"}
--         }
--     },
}
